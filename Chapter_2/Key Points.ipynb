{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Frame the Problem: The first question to ask your boss is what exactly is the business objective;\n",
    "2. Pipelines: A sequence of data processing components is called a data pipeline.\n",
    "3. The next question to ask is what the current solution looks like (if any).\n",
    "4. First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Select a Performance Measure: \n",
    "Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE).\n",
    "\n",
    "- h is your system’s prediction function, also called a hypothesis\n",
    "- RMSE(X,h) is the cost function measured on the set of examples using your hypothesis h.\n",
    "- suppose that there are many outlier districts. In that case, you may consider using the Mean Absolute Error (also called the Average Absolute Deviation bcz RMSE is sensitive to outliers.\n",
    "- Both the RMSE and the MAE are ways to measure the distance between two vectors:\n",
    "- Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: it is the notion of distance you are familiar with. It is also called the ℓ2 norm, noted ∥ · ∥2 (or just ∥ · ∥).\n",
    "- Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1. It is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n",
    "- The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check the Assumptions: Lastly, it is good practice to list and verify the assumptions that were made so far (by you or others); this can catch serious issues early on.\n",
    "\n",
    "\n",
    "For example, the district pricesthat your system outputs are going to be fed into a downstream Machine Learning\n",
    "system, and we assume that these prices are going to be used as such. But what if the downstream system actually converts the prices into categories (e.g., “cheap,” “medium,” or “expensive”) and then uses those categories instead of the prices themselves? In this case, getting the price perfectly right is not important at all; your system just needs to get the category right. If that’s so, then the problem should have been framed as a classification task, not a regression task. You don’t want to find this out after working on a regression system for months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Get the Data\n",
    "8. Create the Workspace: Creating an Isolated Environment\n",
    "9. Load Data\n",
    "10. Take a Quick Look at the Data Structure:  The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n",
    "11. Let’s look at the other fields. The describe() method shows a summary of the numerical attributes.The std row shows the standard deviation, which measures how dispersed the values are\n",
    "12. Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute.\n",
    "13. Create a Test Set : When you estimate the generalization error using the test set, your estimate will be too optimistic and you will launch a system that will not perform as well as expected. This is called data snooping bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data.\n",
    "###### Discover and Visualize the Data to Gain Insights\n",
    "15. Looking for Correlations : Since the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
    "there is a strong positive correlation; for example, the median house value tends to go\n",
    "up when the median income goes up. When the coefficient is close to –1, it means\n",
    "that there is a strong negative correlation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 16. Prepare the Data for Machine Learning Algorithms\n",
    "It’s time to prepare the data for your Machine Learning algorithms. Instead of just\n",
    "doing this manually, you should write functions to do that, for several good reasons:\n",
    "- This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).\n",
    "-  You will gradually build a library of transformation functions that you can reuse in future projects.\n",
    "-  You can use these functions in your live system to transform the new data before feeding it to your algorithms.\n",
    "-  This will make it possible for you to easily try various transformations and see which combination of transformations works best.\n",
    "\n",
    "17. Seperate Features and labels\n",
    "###### 18. Data Cleaning :\n",
    "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n",
    "- Get rid of the corresponding districts.\n",
    "- Get rid of the whole attribute.\n",
    "- Set the values to some value (zero, the mean, the median, etc.).\n",
    "You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()\n",
    "methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer(Handy class for missing values)\n",
    "Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. Here is how to use it. First, you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that\n",
    "attribute:\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\") \n",
    "\n",
    "Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity:\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "Now you can fit the imputer instance to the training data using the fit() method:\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.\n",
    "\n",
    "Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n",
    "\n",
    "imputer.statistics_\n",
    "\n",
    "\n",
    "Now you can use this “trained” imputer to transform the training set by replacing\n",
    "missing values by the learned medians:\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "The result is a plain NumPy array containing the transformed features. If you want to\n",
    "put it back into a Pandas DataFrame, it’s simple:\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Handling Text and Categorical Attributes : \n",
    "Most Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class18:\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Problem: Compareable unorder data\n",
    "a common solution is to create one binary attribute per category: one attribute equal to 1 when the category\n",
    "is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is\n",
    "“INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because\n",
    "only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new\n",
    "attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEn\n",
    "coder class to convert categorical values into one-hot vectors19.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "You can use it mostly like a normal 2D array,20 but if you really want to convert it to a (dense) NumPy array, just call the toarray() method:\n",
    "\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. Feature Scaling: \n",
    "One of the most important transformations you need to apply to your data is feature\n",
    "scaling.\n",
    "- With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min.\n",
    "\n",
    "1. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.\n",
    "\n",
    "2. Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance.Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).standardization is much less affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation Pipelines \n",
    "\n",
    "Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "('imputer', SimpleImputer(strategy=\"median\")),\n",
    "('attribs_adder', CombinedAttributesAdder()),\n",
    "('std_scaler', StandardScaler()),\n",
    "])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have handled the categorical columns and the numerical columns separately.It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20,\n",
    "Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with Pandas DataFrames. Let’s use it to apply all the transformations to the housing data:\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "(\"num\", num_pipeline, num_attribs),\n",
    "(\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Select and Train a Model\n",
    "22. Training and Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeRegressor. This is a powerful model, capable of finding\n",
    "complex nonlinear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
